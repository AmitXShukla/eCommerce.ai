var documenterSearchIndex = {"docs":
[{"location":"process/#Business-Process","page":"Business Process","title":"Business Process","text":"","category":"section"},{"location":"analytics/#Analytics","page":"AI/ML Analytics","title":"Analytics","text":"","category":"section"},{"location":"analytics/#Ad-hoc","page":"AI/ML Analytics","title":"Ad-hoc","text":"","category":"section"},{"location":"analytics/","page":"AI/ML Analytics","title":"AI/ML Analytics","text":"Sales by Region by Period\nSales by Vendor by Region by Period\nSales by Item by Region by Period\nSales by Item category by Region by Period","category":"page"},{"location":"analytics/#Self-Service","page":"AI/ML Analytics","title":"Self Service","text":"","category":"section"},{"location":"analytics/","page":"AI/ML Analytics","title":"AI/ML Analytics","text":"Purchase Orders by Region by Period\nRequisitions by Region by Period","category":"page"},{"location":"analytics/#Advance-Analytics","page":"AI/ML Analytics","title":"Advance Analytics","text":"","category":"section"},{"location":"analytics/","page":"AI/ML Analytics","title":"AI/ML Analytics","text":"would, could, should","category":"page"},{"location":"analytics/#Predictive-Analytics","page":"AI/ML Analytics","title":"Predictive Analytics","text":"","category":"section"},{"location":"analytics/","page":"AI/ML Analytics","title":"AI/ML Analytics","text":"SARIMA Revenue Forecasting\nITEMs bought together\nGuided Buying\nGuided Buying by weather\nGuided Buying by event\nGuided Buying - biased (Promotional)","category":"page"},{"location":"analytics/#Real-time-Analytics","page":"AI/ML Analytics","title":"Real time Analytics","text":"","category":"section"},{"location":"analytics/","page":"AI/ML Analytics","title":"AI/ML Analytics","text":"ITEM On hand  Qty\nReceiving status by Vendor\nReceiving status by Item","category":"page"},{"location":"architecture/#Implementation-approach","page":"Implementation","title":"Implementation approach","text":"","category":"section"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"eCommerce.ai Julia package provide a unified Analytics platform to support data analytical operations on all sort of Sales, Orders | Revenue  Management, Procurement, Accounts Payable, Procurement including Vendor, Use, Freight, Misc Tax Accruals data to address complete Buy to Pay data wrangling operations.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"This package will provide a complete Analytic Software package, which can be deployed as a bolt-on or independent application for all data extract, load, transformation, ad-hoc reporting & Analytics, visualizations and tooling to support Data Science, AI, ML predictive Analytics.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"This package is intended for small, medium, large and very Big Organizations who require a Big Data Tools which can ELT i.e. Extract very large amount of structured and unstructured data, load data into a uniform platform such as RDBMS, Hadoop Data Lake or non-SQL environment.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Further, advance data transformation wrangling techniques can be applied to prepare data for operations reporting, data analytic, advance data visualizations, data science operations including AI, ML for predictions.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"This package also show case reporting, visualizations to support real time, live reporting on all mobile, web devices. ","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"eCommerce.ai takes a methodological business workflow approach (follow data) to solve this challenge.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Step 1:","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"At first, a detail analysis (much of the work) is done to understand, define end-to-end source to pay, order to cash, procure to sell business operations.\nYou will see, tons of examples included in this project, These examples resemble real life commercial good procurement to sales including payments, accruals, receiving and expenses etc.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Step 2:","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Next, 3rd part IOT data like, local community events, holiday calendars, long weekends, weathers, climatic conditions, type of data is gathered.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Step 3:","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Then all of this data is combined, cleaned and wrangled in a format which can be used in Analytics.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Step 4:","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Then after, following Analytics is run and made available (in form of Jupyter | Pluto notebooks) for business operations, KPI Dashboards and Executive dashboards. These KPIs help business leadership take effective operational intelligence decisions.","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Final deliverables","category":"page"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"Ad-Hoc reports :    simple data queries\nAnalytics:          Self service reporting, analytics & visualization\nAdvance Analytics:  would | could | should\nPredictive Analytics:   train, test and predict KPIs\nReal time Analytics:    running analytics on real time data","category":"page"},{"location":"architecture/#Architecture","page":"Implementation","title":"Architecture","text":"","category":"section"},{"location":"architecture/#Proposed-Architecture","page":"Implementation","title":"Proposed Architecture","text":"","category":"section"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"(Image: Proposed Architecture)","category":"page"},{"location":"architecture/#Workflow","page":"Implementation","title":"Workflow","text":"","category":"section"},{"location":"architecture/#Business-process-workflow","page":"Implementation","title":"Business process workflow","text":"","category":"section"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"(Image: Proposed Workflow)","category":"page"},{"location":"architecture/#ERD","page":"Implementation","title":"ERD","text":"","category":"section"},{"location":"architecture/#Physical-ERD-DB-Architecture","page":"Implementation","title":"Physical ERD DB Architecture","text":"","category":"section"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"(Image: Proposed ERD)","category":"page"},{"location":"architecture/#Explainable-AI","page":"Implementation","title":"Explainable AI","text":"","category":"section"},{"location":"architecture/#Explainable-AI-2","page":"Implementation","title":"Explainable AI","text":"","category":"section"},{"location":"architecture/","page":"Implementation","title":"Implementation","text":"(Image: Explainable AI)","category":"page"},{"location":"data/#working-with-Data","page":"Data sets","title":"working with Data","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"In this section, we will review and create, ERP data sets required for system analytics.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"Please see, most of the datasets discussed below, adhere to ERD discussed earlier. These datasets are not real but is very close to real life datasets.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"These scripts do NOT copy any real organization dataset, any resemblance to any real organization working data is pure coincidental.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"note: ERP Systems\neCommerce.ai package supports following ERP systems data structures.Oracle, PeopleSoft, SAP, Tally, Intuit, QuickBooks etc. I will cover examples from ERP Domains like GL (General Ledger), AP (Accounts Payable), AR (Account Receivables), B2P (Buy to Pay), Expense, Travel & Time, HCM Human Capital Management, CRM etc.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"note: Note\nGitHub doesn't allow to load GBs/volumes data into GitHub repository.While most of the scripts used in this project/package is tested on 20+GBs of data. You'll find sample datasets in ../../assets/sampleData directory.Also, you can use following scripts to generate more data by changing sampleSize variable in following scripts.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"Let's get started, and create ERP commerce dataset.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"","category":"page"},{"location":"data/#SUPPLY-CHAIN","page":"Data sets","title":"SUPPLY CHAIN","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"you will need following packages.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using Pkg\nPkg.add(\"DataFrames\")\nPkg.add(\"Dates\")\nPkg.add(\"CategoricalArrays\")\nPkg.add(\"Interact\")\nPkg.add(\"WebIO\")\nPkg.add(\"CSV\")\nPkg.add(\"XLSX\")\nPkg.add(\"DelimitedFiles\")\nPkg.add(\"Distributions\")\nPkg.build(\"WebIO\")\nPkg.status();\n\nusing DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"rest of this blog, I will assume, you have added all packages and imported in current namespace/notebook scope.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"note: Note\nAll of Finance and supply chain data discussed here is also uploaded in GitHub repo under sampleData folder. This same script can be used to produce more voluminous data.","category":"page"},{"location":"data/#Chartfields","page":"Data sets","title":"Chartfields","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"Item master, Item Attribs, Item Costing\nUNSPSC:  The United Nations Standard Products and Services Code® (UNSPSC®) is a global classification system of products and services.               These codes are used to classify products and services.\nGTIN: Global Trade Item Number (GTIN) can be used by a company to uniquely identify all of its trade items. GS1 defines trade items as products or services that are priced, ordered or invoiced at any point in the supply chain.\nVendor master, Vendor Attribs, Vendor Costing   Customer/Buyer/Procurement Officer Attribs   shipto, warehouse, storage & inventory locations","category":"page"},{"location":"data/#Transactions","page":"Data sets","title":"Transactions","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"PurchaseOrder\nMSR - Material Service\nVoucher\nInvoice\nReceipt\nShipment\nSales, Revenue\nTravel, Expense, TimeCard\nAccounting Lines","category":"page"},{"location":"data/#Item-Master-from-UNSPSC","page":"Data sets","title":"Item Master from UNSPSC","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"import Pkg\nPkg.add(\"XLSX\")\nPkg.add(\"CSV\")\nusing XLSX, CSV, DataFrames\n###############################\n## create SUPPLY CHAIN DATA ###\n###############################\n# Item master, Item Attribs, Item Costing ##\n#       UNSPSC, GTIN\n############################################\n\n##########\n# UNSPSC #\n##########\n# UNSPSC file can be downloaded from this link https://www.ungm.org/Public/UNSPSC\n# xf = XLSX.readxlsx(\"../../assets/sampleData/UNGM_UNSPSC_09-Apr-2022..xlsx\")\n# xf will display names of sheets and rows with data\n# let's read this data in to a DataFrame\n\n# using below command will read xlsx data into DataFrame but will not render column labels\n# df = DataFrame(XLSX.readdata(\"../../assets/sampleData/UNGM_UNSPSC_09-Apr-2022..xlsx\", \"UNSPSC\", \"A1:D12988\"), :auto)\ndfUNSPSC = DataFrame(XLSX.readtable(\"../../assets/sampleData/UNGM_UNSPSC_09-Apr-2022..xlsx\", \"UNSPSC\")...)\n# ... operator will splat the tuple (data, column_labels) into the constructor of DataFrame\n\n# replace missing values with an integer 99999\nreplace!(dfUNSPSC.\"Parent key\", missing => 99999)\n\n# let's export this clean csv, we'll load this into database\n# CSV.write(\"UNSPSC.csv\", dfUNSPSC)\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfUNSPSC)\n# empty!(dfUNSPSC)\n# Base.summarysize(dfUNSPSC)\n\nfirst(dfUNSPSC, 5)","category":"page"},{"location":"data/#GTIN-data","page":"Data sets","title":"GTIN data","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n##########\n# GTIN ###\n##########\n\n# xf = XLSX.readxlsx(\"../../assets/sampleData/DS_GTIN_ALL.xlsx\")\n# xf will display names of sheets and rows with data\n# let's read this data in to a DataFrame\n\n# using below command will read xlsx data into DataFrame but will not render column labels\n# df = DataFrame(XLSX.readdata(\"../../assets/sampleData/DS_GTIN_ALL.xlsx\", \"Worksheet\", \"A14:E143403   \"), :auto)\ndfGTIN = DataFrame(XLSX.readtable(\"../../assets/sampleData/DS_GTIN_ALL.xlsx\", \"Worksheet\";first_row=14)...)\nfirst(dfGTIN,5)\n# ... operator will splat the tuple (data, column_labels) into the constructor of DataFrame\n\n# replace missing values with an integer 99999\n# replace!(dfUNSPSC.\"Parent key\", missing => 99999)\n# size(dfUNSPSC)\n\n# let's export this clean csv, we'll load this into database\n# CSV.write(\"UNSPSC.csv\", dfUNSPSC)\n# readdir(pwd())\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfGTIN)\n# empty!(dfGTIN)\n# Base.summarysize(dfGTIN)","category":"page"},{"location":"data/#Vendor-Master","page":"Data sets","title":"Vendor Master","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n#################\n# Vendor master #\n#################\n\n#####\n## This is big data set (<5GB), uncomments and run only once ##\n#####\n# data, header = readdlm(\"../../assets/sampleData/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# show(first(dfGUDIDdevice[:,[:brandName, :catalogNumber, :dunsNumber, :companyName, :rx, :otc]],5), allcols=true)\n\n# create Vendor Master from GUDID dataset\n# show(first(dfGUDIDdevice,5), allcols=true)\n# names(dfGUDIDdevice)\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :catalogNumber, :dunsNumber, :companyName, :rx, :otc]])\n# dfVendor = unique(dfGUDIDdevice[:,[:companyName]]) # 7574 unique vendors\n\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# dfVendor is a good dataset, have 216k rows for 7574 unique vendors\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfVendor)\n# empty!(dfVendor)\n# Base.summarysize(dfVendor)","category":"page"},{"location":"data/#Locations","page":"Data sets","title":"Locations","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n#### Location Master\n\ndata, header = readdlm(\"../../assets/sampleData/uscities.csv\", ',', header=true)\ndfLocation = DataFrame(data, vec(header))\nfirst(dfLocation[:,Not(:zips)],5)\n\n# # remember to empty dataFrame after usage\n# # Julia will flush it out automatically after session,\n# # but often ERP data gets bulky during session\n# Base.summarysize(dfLocation)\n# empty!(dfLocation)\n# Base.summarysize(dfLocation)","category":"page"},{"location":"data/#ORG","page":"Data sets","title":"ORG","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\ndfOrgMaster = DataFrame(\n    ORG=repeat([\"Galp Inc\"], inner=8),\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"])\nfirst(dfOrgMaster,5)\n","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"now since we created Supply chain attribute, chartfields / dimensions","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"item master\nvendor master\nlocation master\norg Hierarchy","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using above chartfields, let's create following Supply Chain Transactions","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"MSR - Material Service request\nPurchaseOrder\nVoucher\nInvoice\nReceipt\nShipment\nSales, Revenue\nTravel, Expense, TimeCard\nAccounting Lines","category":"page"},{"location":"data/#MSR-Material-Service-request","page":"Data sets","title":"MSR - Material Service request","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"../../assets/sampleData/device.txt\", '|', header=true)\n# df GUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"../../assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\n# dfOrgMaster = DataFrame(\n#    ENTITY=repeat([\"HeadOffice\"], inner=8),\n#    GROUP=repeat([\"Operations\"], inner=8),\n#    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n#    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"])\n\n# dfMSR = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    MSR_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    FROM_UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    TO_UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(dfOrgMaster.UNIT, sampleSize));\n# first(dfMSR, 5)","category":"page"},{"location":"data/#PO-Purchase-Order","page":"Data sets","title":"PO - Purchase Order","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"../../assets/sampleData/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"../../assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\ndfOrgMaster = DataFrame(\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"]);\n\n# dfPO = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    PO_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    VENDOR=rand(unique(dfVendor.companyName), sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(1:150, sampleSize),\n#    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n#    );\n# show(first(dfPO, 5),allcols=true)","category":"page"},{"location":"data/#Invoice-Voucher-Invoice","page":"Data sets","title":"Invoice - Voucher Invoice","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"../../assets/sampleData/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"../../assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\ndfOrgMaster = DataFrame(\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"]);\n\n# dfVCHR = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    VCHR_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    STATUS=rand([\"Closed\",\"Paid\",\"Open\",\"Cancelled\",\"Exception\"], sampleSize),\n#    VENDOR_INVOICE_NUM = rand(10001:9999999, sampleSize),\n#    VENDOR=rand(unique(dfVendor.companyName), sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(1:150, sampleSize),\n#    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n#    );\n# show(first(dfVCHR, 5),allcols=true)","category":"page"},{"location":"data/#Sales-Revenue-Register","page":"Data sets","title":"Sales - Revenue Register","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\nsampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"../../assets/sampleData/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"../../assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\ndfOrgMaster = DataFrame(\n    ENTITY=repeat([\"HeadOffice\"], inner=8),\n    GROUP=repeat([\"Operations\"], inner=8),\n    DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"]);\n\n# dfREVENUE = DataFrame(\n#    UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n#    SALES_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n#    STATUS=rand([\"Sold\",\"Pending\",\"Hold\",\"Cancelled\",\"Exception\"], sampleSize),\n#    SALES_RECEIPT_NUM = rand(10001:9999999, sampleSize),\n#    CUSTOMER=rand(unique(dfVendor.companyName), sampleSize),\n#    GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n#    QTY = rand(1:150, sampleSize),\n#    UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n#    );\n# show(first(dfREVENUE, 5),allcols=true)","category":"page"},{"location":"data/#Shipment-Receipt","page":"Data sets","title":"Shipment - Receipt","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"# using DataFrames, Dates, Interact, CategoricalArrays, WebIO, CSV, XLSX, DelimitedFiles, Distributions\n# sampleSize = 1000 # number of rows, scale as needed\n\n# data, header = readdlm(\"../../assets/sampleData/device.txt\", '|', header=true)\n# dfGUDIDdevice = DataFrame(data, vec(header))\n# dfVendor = unique(dfGUDIDdevice[:,[:brandName, :dunsNumber, :companyName, :rx, :otc]])\n# data, header = readdlm(\"../../assets/sampleData/uscities.csv\", ',', header=true)\n# dfLocation = DataFrame(data, vec(header))\n# dfOrgMaster = DataFrame(\n    # ENTITY=repeat([\"HeadOffice\"], inner=8),\n    # GROUP=repeat([\"Operations\"], inner=8),\n    # DEPARTMENT=[\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"Procurement\",\"HR\",\"HR\",\"MFG\"],\n    # UNIT=[\"Sourcing\",\"Sourcing\",\"Maintenance\",\"Support\",\"Services\",\"Helpdesk\",\"ServiceCall\",\"IT\"])\n\n# dfSHIPRECEIPT = DataFrame(\n   # UNIT = rand(dfOrgMaster.UNIT, sampleSize),\n   # SHIP_DATE=rand(collect(Date(2020,1,1):Day(1):Date(2022,5,1)), sampleSize),\n   # STATUS=rand([\"Shipped\",\"Returned\",\"In process\",\"Cancelled\",\"Exception\"], sampleSize),\n   # SHIPMENT_NUM = rand(10001:9999999, sampleSize),\n   # CUSTOMER=rand(unique(dfVendor.companyName), sampleSize),\n   # GUDID = rand(dfGUDIDdevice.PrimaryDI, sampleSize),\n   # QTY = rand(1:150, sampleSize),\n   # UNIT_PRICE = rand(Normal(100, 2), sampleSize)\n   # );\n# show(first(dfSHIPRECEIPT, 5),allcols=true)","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"","category":"page"},{"location":"data/#FINANCE-Data-model","page":"Data sets","title":"FINANCE Data model","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"Chart of accounts (organized hierarchy of account groups in tree form), Location/Department or Product based hierarchy allows businesses to group and report organization activities based on business processes.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"These hierarchical grouping help capture monetary and statistical values of organization in finance statements.","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"","category":"page"},{"location":"data/","page":"Data sets","title":"Data sets","text":"To create Finance Data model and  Ledger Cash-flow or Balance Sheet like statements, We need associated dimensions (chartfields like chart of accounts).","category":"page"},{"location":"data/#Accounts-chartfield","page":"Data sets","title":"Accounts chartfield","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using Pkg, DataFrames, CategoricalArrays, PooledArrays, Dates\n\n# here CLASSIFICATION column vector stores 3500 distinct values in an array\nCLASSIFICATION=repeat([\"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n                ], inner=500)\n\ncl = categorical(CLASSIFICATION)\nlevels(cl)\n\n# using PooledArrays\npl = categorical(CLASSIFICATION)\nlevels(pl)\n\n# show values in tabular format\n# run one command at a time\ndf = DataFrame(Dict(\"Descr\" => \"CLASSIFICATION...ARR...\", \"Value\" => size(CLASSIFICATION)[1]))\npush!(df,(\"CAT...ARR...\",size(cl)[1]))\npush!(df,(\"CAT...ARR..COMPRESS.\",size(compress(cl))[1]))\npush!(df,(\"POOL...ARR...\",size(pl)[1]))\npush!(df,(\"POOL...ARR..COMPRESS.\",size(compress(pl))[1]))\npush!(df,(\"CAT...LEVELs...\",size(levels(cl))[1]))\npush!(df,(\"POOL...LEVELs...\",size(levels(pl))[1]))\npush!(df,(\"CLASSIFICATION...MEMSIZE\", Base.summarysize(CLASSIFICATION)))\npush!(df,(\"CAT...ARR...MEMSIZE\", Base.summarysize(cl)))\npush!(df,(\"POOL...ARR...MEMSIZE\", Base.summarysize(pl)))\npush!(df,(\"CAT...ARR..COMPRESS...MEMSIZE\", Base.summarysize(compress(cl))))\npush!(df,(\"POOL...ARR..COMPRESS...MEMSIZE\", Base.summarysize(compress(pl))))\n\nfirst(df,5)\n\naccountsDF = DataFrame(\n    ENTITY = \"Galp Retail Inc.\",\n    AS_OF_DATE=Date(\"1900-01-01\", dateformat\"y-m-d\"),\n    ID = 11000:1000:45000,\n    CLASSIFICATION=repeat([\n        \"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n                ], inner=5),\n    CATEGORY=[\n        \"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n        \"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n        \"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n        \"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n        \"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n        \"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n        \"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"],\n    STATUS=\"A\",\n    DESCR=repeat([\n    \"operating expenses\",\"non-operating expenses\",\"ASSETS\",\"liability\",\"net-worth\",\"stats\",\"revenue\"], inner=5),\n    ACCOUNT_TYPE=repeat([\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"],inner=5))\naccountsDF[collect(1:5),:]","category":"page"},{"location":"data/#Department-chartfield","page":"Data sets","title":"Department chartfield","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates\n## create Accounts chartfield\n# DEPARTMENT Chartfield\ndeptDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 1100:100:1500,\n    CLASSIFICATION=[\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"],\n    CATEGORY=[\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"],\n    STATUS=\"A\",\n    DESCR=[\n    \"Sales & Marketing\",\"Human Resource\",\"Information Technology\",\"Business leaders\",\"other temp\"\n        ],\n    DEPT_TYPE=[\"S\",\"H\",\"I\",\"B\",\"O\"]);\ndeptDF[collect(1:5),:]","category":"page"},{"location":"data/#Location-chartfield","page":"Data sets","title":"Location chartfield","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"using DataFrames, Dates\nlocationDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 11:1:22,\n    CLASSIFICATION=repeat([\n        \"Galp Region A\",\"Galp Region B\", \"Galp Region C\"], inner=4),\n    CATEGORY=repeat([\n        \"Galp Region A\",\"Galp Region B\", \"Galp Region C\"], inner=4),\n    STATUS=\"A\",\n    DESCR=[\n\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n\"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n\"Dallas\",\"San Francisco\"],\n    LOC_TYPE=\"Physical\");\nlocationDF[:,:]","category":"page"},{"location":"data/#Ledger-RECORD","page":"Data sets","title":"Ledger RECORD","text":"","category":"section"},{"location":"data/","page":"Data sets","title":"Data sets","text":"## pu\nusing DataFrames, Dates\naccountsDF = DataFrame(\n    ENTITY = \"Galp Retail Inc.\",\n    AS_OF_DATE=Date(\"1900-01-01\", dateformat\"y-m-d\"),\n    ID = 11000:1000:45000,\n    CLASSIFICATION=repeat([\n        \"OPERATING_EXPENSES\",\"NON-OPERATING_EXPENSES\", \"ASSETS\",\"LIABILITIES\",\"NET_WORTH\",\"STATISTICS\",\"REVENUE\"\n                ], inner=5),\n    CATEGORY=[\n        \"Travel\",\"Payroll\",\"non-Payroll\",\"Allowance\",\"Cash\",\n        \"Facility\",\"Supply\",\"Services\",\"Investment\",\"Misc.\",\n        \"Depreciation\",\"Gain\",\"Service\",\"Retired\",\"Fault.\",\n        \"Receipt\",\"Accrual\",\"Return\",\"Credit\",\"ROI\",\n        \"Cash\",\"Funds\",\"Invest\",\"Transfer\",\"Roll-over\",\n        \"FTE\",\"Members\",\"Non_Members\",\"Temp\",\"Contractors\",\n        \"Sales\",\"Merchant\",\"Service\",\"Consulting\",\"Subscriptions\"],\n    STATUS=\"A\",\n    DESCR=repeat([\n    \"operating expenses\",\"non-operating expenses\",\"ASSETS\",\"liability\",\"net-worth\",\"stats\",\"revenue\"], inner=5),\n    ACCOUNT_TYPE=repeat([\"E\",\"E\",\"A\",\"L\",\"N\",\"S\",\"R\"],inner=5))\n# DEPARTMENT Chartfield\ndeptDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 1100:100:1500,\n    CLASSIFICATION=[\"SALES\",\"HR\", \"IT\",\"BUSINESS\",\"OTHERS\"],\n    CATEGORY=[\"sales\",\"human_resource\",\"IT_Staff\",\"business\",\"others\"],\n    STATUS=\"A\",\n    DESCR=[\n    \"Sales & Marketing\",\"Human Resource\",\"Information Technology\",\"Business leaders\",\"other temp\"\n        ],\n    DEPT_TYPE=[\"S\",\"H\",\"I\",\"B\",\"O\"]);\nsize(deptDF),deptDF[collect(1:5),:]\n\nlocationDF = DataFrame(\n    AS_OF_DATE=Date(\"2000-01-01\", dateformat\"y-m-d\"), \n    ID = 11:1:22,\n    CLASSIFICATION=repeat([\n        \"Galp Region A\",\"Galp Region B\", \"Galp Region C\"], inner=4),\n    CATEGORY=repeat([\n        \"Galp Region A\",\"Galp Region B\", \"Galp Region C\"], inner=4),\n    STATUS=\"A\",\n    DESCR=[\n\"Boston\",\"New York\",\"Philadelphia\",\"Cleveland\",\"Richmond\",\n\"Atlanta\",\"Chicago\",\"St. Louis\",\"Minneapolis\",\"Kansas City\",\n\"Dallas\",\"San Francisco\"],\n    LOC_TYPE=\"Physical\");\nlocationDF[:,:]\n\n# creating Ledger\nledgerDF = DataFrame(\n            LEDGER = String[], FISCAL_YEAR = Int[], PERIOD = Int[], ORGID = String[],\n            OPER_UNIT = String[], ACCOUNT = Int[], DEPT = Int[], LOCATION = Int[],\n            POSTED_TOTAL = Float64[]\n            );\n\n# create 2020 Period 1-12 Actuals Ledger \nl = \"Actuals\";\nfy = 2020;\nfor p = 1:12\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"Galp Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# create 2021 Period 1-4 Actuals Ledger \nl = \"Actuals\";\nfy = 2021;\nfor p = 1:4\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"Galp Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# create 2021 Period 1-4 Budget Ledger \nl = \"Budget\";\nfy = 2021;\nfor p = 1:12\n    for i = 1:10^5\n        push!(ledgerDF, (l, fy, p, \"Galp Inc.\", rand(locationDF.CATEGORY),\n            rand(accountsDF.ID), rand(deptDF.ID), rand(locationDF.ID), rand()*10^8))\n    end\nend\n\n# here is ~3 million rows ledger dataframe\nfirst(ledgerDF,5)","category":"page"},{"location":"#about-e-commerce.ai","page":"Introduction","title":"about e-commerce.ai","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"eCommerce.ai is a complete end-to-end Data Science, Graph, AI/ML Analytics technology framework to support data gathering, mining, wrangling, analysis including visualization & performing AI/ML Analytics on data, with intent to understand, support and predict eCommerce operations to support daily business operations for small, medium and very large organizations.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"warning: Galp's Hackathon Retail 4.0\neCommerce.ai is an official project submitted as Galp's Hackathon Retail 4.0 Hackathon project work.  This documentation is published in GitHub gh-pages branch and does include executed Code & examples. Complete source code is also available under main GitHub Branch.","category":"page"},{"location":"#Author","page":"Introduction","title":"Author","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Author\nAuthor: Amit ShuklaBio: about meLast Update Date: May 20 2022Who should read this: ERP Consultants working with small, medium, large orgsVersion: 0.22Sponsorship: open for funding","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"(Image: GitHub) (Image: YouTube) (Image: Twitter) (Image: LinkedIn) (Image: Medium)","category":"page"},{"location":"#Platform","page":"Introduction","title":"Platform","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"info: Platform\nFrontend: Julia 1.7.1Backend: Oracle OCI Cloud, Oracle ADW (Autonomous data warehouse) | TigerGraph/Oracle Graph DBRest API: Julia, TGCloud RESTAPIAI: Julia, FLUXml.ai, Oracle AutoML","category":"page"},{"location":"#how-to-use-this-book","page":"Introduction","title":"how to use this book","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"This book first version is completely free(v1.2) and is published as website under GitHub gh-pages branch.","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Most of the source code is MIT License, (except few ML/Deep Learning algorithms, which are proprietary and customer owned content).","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"Complete source code can be found here. https://github.com/AmitXShukla/eCommerce.ai","category":"page"},{"location":"#Table-of-Contents","page":"Introduction","title":"Table of Contents","text":"","category":"section"},{"location":"","page":"Introduction","title":"Introduction","text":"pages=[\n    \"index.md\"\n    \"demo.md\"\n    \"architecture.md\"\n    \"implementation.md\"\n    \"process.md\"\n    \"workflow.md\"\n    \"explain.md\"\n    \"data.md\"\n    \"analytics.md\"\n    \"adhoc.md\"\n    \"advance.md\"\n    \"prediction.md\"\n    \"realtime.md\"\n    ]","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"","category":"page"},{"location":"","page":"Introduction","title":"Introduction","text":"warning: ERP Systems\neCommerce.ai package supports following ERP systems data structures.Oracle, PeopleSoft, SAP, Tally, Intuit, QuickBooks etc. I will cover examples from ERP Domains like GL (General Ledger), AP (Accounts Payable), AR (Account Receivables), B2P (Buy to Pay), Expense, Travel & Time, HCM Human Capital Management, CRM etc.","category":"page"},{"location":"demo/#DEMO-notebooks","page":"Demo","title":"DEMO notebooks","text":"","category":"section"},{"location":"demo/","page":"Demo","title":"Demo","text":"These notebooks are best run in Julia lang Pluto notebooks, as Pluto provide a real-time reactive display.","category":"page"},{"location":"demo/","page":"Demo","title":"Demo","text":"However, following examples are shown in Jupyter notebooks, as this is for demo purpose. And GitHub renders Jupyter notebooks in repo itself.","category":"page"},{"location":"demo/#Ad-hoc","page":"Demo","title":"Ad-hoc","text":"","category":"section"},{"location":"demo/","page":"Demo","title":"Demo","text":"Sales by Region by Period\nSales by Vendor by Region by Period\nSales by Item by Region by Period\nSales by Item category by Region by Period","category":"page"},{"location":"demo/#Self-Service","page":"Demo","title":"Self Service","text":"","category":"section"},{"location":"demo/","page":"Demo","title":"Demo","text":"Purchase Orders by Region by Period\nRequisitions by Region by Period","category":"page"},{"location":"demo/#Advance-Analytics","page":"Demo","title":"Advance Analytics","text":"","category":"section"},{"location":"demo/","page":"Demo","title":"Demo","text":"would, could, should","category":"page"},{"location":"demo/#Predictive-Analytics","page":"Demo","title":"Predictive Analytics","text":"","category":"section"},{"location":"demo/","page":"Demo","title":"Demo","text":"SARIMA Revenue Forecasting\nITEMs bought together\nGuided Buying\nGuided Buying by weather\nGuided Buying by event\nGuided Buying - biased (Promotional)","category":"page"},{"location":"demo/#Real-time-Analytics","page":"Demo","title":"Real time Analytics","text":"","category":"section"},{"location":"demo/","page":"Demo","title":"Demo","text":"ITEM On hand  Qty\nReceiving status by Vendor\nReceiving status by Item","category":"page"},{"location":"iot/#Internet-of-Things","page":"IOTs","title":"Internet of Things","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"IOT data is one very important aspect of eCommerce. Climatic conditions, weather, festive seasons, events etc. often significantly impact business operations.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"Sometimes, it may not be always possible to plan for unplanned catastrophic events, but whenever data is available in advance, it always help tremendously in planning and provide huge opportunity for growth in revenue, branding and building relationships with customer.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"Purpose of this section is, to use, weather | climatic | planned events data to help detect \"Seasonality\" in SARIMA ML Models in AI predictive analytics.  (SARIMA : seasonal auto regression integrated moving average time series machine learning model).","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"We will use standard weather and news information from weather.com and NEW API websites.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"below are few example datasets gathered via getPiper IOT device located in Los Angeles CA to support our analysis.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"This section, assume, implanted IOT sensors or available API to capture vision, density, temperature, moisture, sound and activities.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"not all IOT data mentioned below is used in our analysis.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#events","page":"IOTs","title":"events","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"using CSV, DataFrames\neventDF = DataFrame(CSV.File(\"../../assets/sampleData/eventCalendar.csv\"))\nfirst(eventDF[:, :], 10)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#holiday-calendar","page":"IOTs","title":"holiday calendar","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"using CSV, DataFrames\ncalendarDF = DataFrame(CSV.File(\"../../assets/sampleData/calendar.csv\"))\nfirst(calendarDF[:, :], 10)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#temperature","page":"IOTs","title":"temperature","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"Outside temperature","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"(Image: Sensors)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"##############################################\n# outdoorTemp\n##############################################\nusing DataFrames, CSV, Dates, Distributions\nsampleSize = 365\nweatherDF = DataFrame(\n    recordDate = Date(\"2022-01-01\", dateformat\"y-m-d\"):Day(1):(Date(\"2022-01-01\", dateformat\"y-m-d\")+ Day(sampleSize-1)),\n    cityId = 1:1:sampleSize, \n    state = rand([\"LA\",\"LA\",\"FL\"], sampleSize),\n    indoorTemp = rand(64:1:94, sampleSize),\n    outdoorTemp = rand(54:1:104, sampleSize),\n    wind = rand(5:1:30, sampleSize),\n    humidity = rand(30:1:70, sampleSize),\n    precipitation = rand(0:1:5, sampleSize)\n    )\nfirst(weatherDF[:, [:recordDate, :cityId, :state, :outdoorTemp]], 10)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"Inside temperature","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"(Image: Sensors)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"##############################################\n# inside temperature\n##############################################\nusing DataFrames, CSV, Dates, Distributions\nsampleSize = 365\nweatherDF = DataFrame(\n    recordDate = Date(\"2022-01-01\", dateformat\"y-m-d\"):Day(1):(Date(\"2022-01-01\", dateformat\"y-m-d\")+ Day(sampleSize-1)),\n    cityId = 1:1:sampleSize, \n    state = rand([\"LA\",\"LA\",\"FL\"], sampleSize),\n    indoorTemp = rand(64:1:94, sampleSize),\n    outdoorTemp = rand(54:1:104, sampleSize),\n    wind = rand(5:1:30, sampleSize),\n    humidity = rand(30:1:70, sampleSize),\n    precipitation = rand(0:1:5, sampleSize)\n    )\nfirst(weatherDF[:, [:recordDate, :cityId, :state, :outdoorTemp]], 10)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#moisture","page":"IOTs","title":"moisture","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"(Image: Sensors)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"##############################################\n# moisture\n##############################################\nusing DataFrames, CSV, Dates, Distributions\nsampleSize = 365\nweatherDF = DataFrame(\n    recordDate = Date(\"2022-01-01\", dateformat\"y-m-d\"):Day(1):(Date(\"2022-01-01\", dateformat\"y-m-d\")+ Day(sampleSize-1)),\n    cityId = 1:1:sampleSize, \n    state = rand([\"LA\",\"LA\",\"FL\"], sampleSize),\n    indoorTemp = rand(64:1:94, sampleSize),\n    outdoorTemp = rand(54:1:104, sampleSize),\n    wind = rand(5:1:30, sampleSize),\n    humidity = rand(30:1:70, sampleSize),\n    precipitation = rand(0:1:5, sampleSize)\n    )\nfirst(weatherDF[:,[:recordDate,:cityId, :state, :humidity]], 10)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#sound","page":"IOTs","title":"sound","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"(Image: Sensors)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"##############################################\n# Noise / sound\n##############################################\nusing DataFrames, CSV, Dates, Distributions\nsampleSize = 365\nweatherDF = DataFrame(\n    recordDate = Date(\"2022-01-01\", dateformat\"y-m-d\"):Day(1):(Date(\"2022-01-01\", dateformat\"y-m-d\")+ Day(sampleSize-1)),\n    cityId = 1:1:sampleSize, \n    state = rand([\"LA\",\"LA\",\"FL\"], sampleSize),\n    indoorTemp = rand(64:1:94, sampleSize),\n    sound = rand(54:1:104, sampleSize),\n    wind = rand(5:1:30, sampleSize),\n    humidity = rand(30:1:70, sampleSize),\n    precipitation = rand(0:1:5, sampleSize)\n    )\nfirst(weatherDF[:,[:recordDate,:cityId, :state, :sound]], 10)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#activities","page":"IOTs","title":"activities","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"motion activities","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"(Image: Sensors)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"##############################################\n# motion activities\n##############################################\nusing DataFrames, CSV, Dates, Distributions\nsampleSize = 365\nweatherDF = DataFrame(\n    recordDate = Date(\"2022-01-01\", dateformat\"y-m-d\"):Day(1):(Date(\"2022-01-01\", dateformat\"y-m-d\")+ Day(sampleSize-1)),\n    cityId = 1:1:sampleSize, \n    state = rand([\"LA\",\"LA\",\"FL\"], sampleSize),\n    indoorTemp = rand(64:1:94, sampleSize),\n    sound = rand(54:1:104, sampleSize),\n    shadows = rand(5:1:30, sampleSize),\n    humidity = rand(30:1:70, sampleSize),\n    precipitation = rand(0:1:5, sampleSize)\n    )\nfirst(weatherDF[:,[:recordDate, :cityId, :state, :shadows]], 10)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#density-/-vision","page":"IOTs","title":"density / vision","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"Capturing crowd density varies case by case. For example, In case of, organized gatherings like Theme parks, concerts etc. number of people attending the event can be predicted in advance.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"However, at the same, number of people entering, leaving and present at a given time is very important. Most of the chaos, happens when too many people appear at a place at the same time. for example, people entering / leaving premises.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"(Image: Sensors) (Image: Sensors)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"Reference","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"other option is, Density will calculated using vision AI, captured through a motion camera / CCTV footage.","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"(Image: Sensors)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#visibility","page":"IOTs","title":"visibility","text":"","category":"section"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"Brightness","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"(Image: Sensors)","category":"page"},{"location":"iot/","page":"IOTs","title":"IOTs","text":"","category":"page"},{"location":"iot/#news","page":"IOTs","title":"news","text":"","category":"section"}]
}
